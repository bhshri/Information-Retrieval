{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Shripad\n",
      "[nltk_data]     Bhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Shripad\n",
      "[nltk_data]     Bhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Shripad\n",
      "[nltk_data]     Bhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Shripad Bhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "#perform preprocessing\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tag_dict = {\"J\": wordnet.ADJ,\"N\": wordnet.NOUN,\"V\": wordnet.VERB,\"R\": wordnet.ADV}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting all the documents for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'FIRE_Dataset_EN_2010\\\\TELEGRAPH_UTF8'\n",
    "\n",
    "\n",
    "\n",
    "files = []\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.utf8' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125586"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to parse the xml documents\n",
    "def parseXML(xmlFile):\n",
    "  tree = ET.parse(xmlFile) \n",
    "  document_id = \"\"\n",
    "  document_text = \"\"\n",
    "  root = tree.getroot()\n",
    "  for  elem in root:\n",
    "    if elem.tag == \"DOCNO\":\n",
    "      document_id = elem.text\n",
    "    elif elem.tag == \"TEXT\":\n",
    "      document_text = elem.text\n",
    "\n",
    "\n",
    "  return document_id,document_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def removeTagsPunctuation(text):\n",
    "        x = re.sub(\"[\\.\\?:,\\\\n/()!<$%\\*\\-+>\\\"\\'\\[\\];]\",\" \",text)\n",
    "        x = re.sub(\"[0-9]\",\" \",x)\n",
    "        return x\n",
    "    \n",
    "    def decontracted(phrase):\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        return phrase\n",
    "    \n",
    "    def removeStopWordsAndLemmatize(text):\n",
    "\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        word_tokens = word_tokenize(text) \n",
    "\n",
    "        final_sentence = []\n",
    "\n",
    "        #performing lemmatization using tags associated with the words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tagged_sentence = nltk.pos_tag(word_tokens)\n",
    "        final_sentence = [lemmatizer.lemmatize(word,tag_dict.get(tag[0].upper(),wordnet.NOUN)) for word,tag in tagged_sentence]   \n",
    "\n",
    "        filtered_sentence = [w for w in final_sentence if not w in stop_words] \n",
    "        return \" \".join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = []\n",
    "doc_texts = []\n",
    "\n",
    "for fileName in files:\n",
    "  try:\n",
    "    doc_id,doc_text = parseXML(fileName)\n",
    "  except:\n",
    "    print(\"error\")    \n",
    "  doc_ids.append(doc_id)\n",
    "  doc_texts.append(doc_text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data frame of all the documents\n",
    "data = pd.DataFrame(list(zip(doc_ids,doc_texts)),columns=['DOCNO','TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing pre processing for all the documents\n",
    "preprocessed_text_column = []\n",
    "for i in range(data.shape[0]):\n",
    "    text_column = data['TEXT'][i]\n",
    "    text_column_decontracted = decontracted(text_column.lower())\n",
    "    text_column_punctuation_removed = removeTagsPunctuation(text_column_decontracted)\n",
    "    text_column_lemmatized = removeStopWordsAndLemmatize(text_column_punctuation_removed)\n",
    "    preprocessed_text_column.append(text_column_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = pd.DataFrame(list(zip(doc_ids,preprocessed_text_column)),columns=['DOCNO','TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to parse all the queries\n",
    "def parseQueryXML(xml_file):\n",
    "  tree = ET.parse(xml_file) \n",
    "  query_numbers = []\n",
    "  query_descriptions = []\n",
    "  root = tree.getroot()\n",
    "  for  elem in root:\n",
    "    if elem.tag == \"top\":\n",
    "      for children in elem:\n",
    "        if children.tag == \"num\":\n",
    "            query_numbers.append(children.text)\n",
    "        elif children.tag == \"desc\":\n",
    "            query_descriptions.append(children.text)\n",
    "\n",
    "  return query_numbers,query_descriptions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_numbers,query_descriptions = parseQueryXML('FIRE_Dataset_EN_2010\\\\en.topics.76-125.2010.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre processing for the queries\n",
    "query_desc_preprocessed = []\n",
    "for i in range(len(query_descriptions)):\n",
    "    text_query = query_descriptions[i]\n",
    "    text_query_decontracted = decontracted(text_query.lower())\n",
    "    text_query_punctuation_removed = removeTagsPunctuation(text_query_decontracted)\n",
    "    text_query_lemmatized = removeStopWordsAndLemmatize(text_query_punctuation_removed)\n",
    "    query_desc_preprocessed.append(text_query_lemmatized)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125586, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data = pd.DataFrame(list(zip(query_numbers,query_desc_preprocessed)),columns=['query_no','query_desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_no</th>\n",
       "      <th>query_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76</td>\n",
       "      <td>reason behind protest meena leader inclusion g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77</td>\n",
       "      <td>attack hezbollah guerrilla indian israeli force</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78</td>\n",
       "      <td>conflict ashok singhal president vishwa hindu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79</td>\n",
       "      <td>plan build road china mount everest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80</td>\n",
       "      <td>initiation legal proceeding advani involvement...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_no                                         query_desc\n",
       "0       76  reason behind protest meena leader inclusion g...\n",
       "1       77    attack hezbollah guerrilla indian israeli force\n",
       "2       78  conflict ashok singhal president vishwa hindu ...\n",
       "3       79                plan build road china mount everest\n",
       "4       80  initiation legal proceeding advani involvement..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Creating tfidf matrix for all the documents\n",
    "tfIdfVectorizer = TfidfVectorizer()\n",
    "tfIdfVectorizer.fit(preprocessed_data['TEXT'].values)\n",
    "tfidf_matrix = tfIdfVectorizer.transform(preprocessed_data['TEXT'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125586, 221939)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing truncated SVD decomposition with only 250 components\n",
    "from scipy.sparse.linalg import svds\n",
    "U, s, V = svds(tfidf_matrix.T,k = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((221939, 250), (250,), (250, 125586))"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape,s.shape,V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating tfidf matrix for the queries\n",
    "query_tfidf = tfIdfVectorizer.transform(query_data.head(5)['query_desc'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 221939)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "s_diag = np.diag(s)\n",
    "s_inverse = inv(s_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new query vector from the U,s obtained from SVD decomposition\n",
    "truncated_query = []\n",
    "for query in query_tfidf:\n",
    "    q = np.dot(np.dot(query.toarray(),U),s_inverse)\n",
    "    truncated_query.append(q)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_T = V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125586, 250)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#performing similarity check between queries and documents and finding top 5 relevant documents\n",
    "\n",
    "query_relevant_documents = dict()\n",
    "for k in range(len(truncated_query)):\n",
    "    query = truncated_query[k]\n",
    "    similarity_values = []\n",
    "    for i in range(v_T.shape[0]):\n",
    "        value = cosine_similarity(query,[v_T[i]])\n",
    "        similarity_values.append([value,i])\n",
    "        \n",
    "    similarity_values.sort(reverse = True)\n",
    "    top_5 = similarity_values[0:5]\n",
    "    query_text = query_desc_preprocessed[k]\n",
    "    query_num = query_data['query_no'][k]\n",
    "    doc_indices = [ i[1] for i in top_5 ]\n",
    "    relevant_docs = data['TEXT'][doc_indices]\n",
    "    relevant_doc_ids = data['DOCNO'][doc_indices]\n",
    "    query_relevant_documents[query_num] = [query_text,relevant_doc_ids,relevant_docs]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"FIRE_Dataset_EN_2010\\\\en.qrels.76-125.2010.txt\", \"r\")\n",
    "l = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual relevant documents for each of the queries\n",
    "\n",
    "query_rel_doc = dict()\n",
    "for i in l:\n",
    "    q_list = i.split()\n",
    "    if q_list[0] > '80':\n",
    "        break\n",
    "    if i[len(i) - 2] == '1':\n",
    "        query_num = q_list[0]\n",
    "        if query_rel_doc.get(query_num) == None:\n",
    "            query_rel_doc[query_num] = [ q_list[2]]\n",
    "        else:\n",
    "            li = query_rel_doc[query_num]\n",
    "            li.append(q_list[2])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Number:  76\n",
      "Documents retrieved using LSI: \n",
      "79779        1060703_nation_story_6430226.utf8\n",
      "81176        1060922_nation_story_6778378.utf8\n",
      "79019        1060521_nation_story_6250723.utf8\n",
      "79589        1060622_nation_story_6385113.utf8\n",
      "110663    1070604_frontpage_story_7872841.utf8\n",
      "Name: DOCNO, dtype: object\n",
      "\n",
      "Relevant documents: \n",
      "1070530_nation_story_7849973.utf8\n",
      "1070602_nation_story_7865940.utf8\n",
      "1070603_nation_story_7869357.utf8\n",
      "1070611_nation_story_7906812.utf8\n",
      "\n",
      "Number of relevant documents retrieved:  0\n",
      "Relevant documents retrieved using LSI:\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Query Number:  77\n",
      "Documents retrieved using LSI: \n",
      "75502    1060720_frontpage_story_6501523.utf8\n",
      "84389      1060724_opinion_story_6493371.utf8\n",
      "35093      1051229_foreign_story_5657775.utf8\n",
      "73002      1060714_foreign_story_6477224.utf8\n",
      "73027      1060717_foreign_story_6488684.utf8\n",
      "Name: DOCNO, dtype: object\n",
      "\n",
      "Relevant documents: \n",
      "1050110_foreign_story_4234881.utf8\n",
      "1060714_foreign_story_6477224.utf8\n",
      "1060715_foreign_story_6482002.utf8\n",
      "1060716_foreign_story_6485226.utf8\n",
      "1060717_foreign_story_6488684.utf8\n",
      "1060719_foreign_story_6496938.utf8\n",
      "1060720_foreign_story_6501311.utf8\n",
      "1060720_frontpage_story_6501523.utf8\n",
      "1060721_foreign_story_6506359.utf8\n",
      "1060727_foreign_index.utf8\n",
      "1060727_foreign_story_6531510.utf8\n",
      "1060802_foreign_story_6557944.utf8\n",
      "1060804_foreign_index.utf8\n",
      "1060804_foreign_story_6567126.utf8\n",
      "1060805_foreign_story_6571528.utf8\n",
      "1060806_foreign_story_6575214.utf8\n",
      "1060807_foreign_story_6578452.utf8\n",
      "1060809_foreign_story_6588005.utf8\n",
      "1060810_foreign_story_6592351.utf8\n",
      "1060814_foreign_story_6609200.utf8\n",
      "\n",
      "Number of relevant documents retrieved:  3\n",
      "Relevant documents retrieved using LSI:\n",
      "1060714_foreign_story_6477224.utf8\n",
      "1060717_foreign_story_6488684.utf8\n",
      "1060720_frontpage_story_6501523.utf8\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Query Number:  78\n",
      "Documents retrieved using LSI: \n",
      "42850     1051117_nation_story_5485590.utf8\n",
      "40374     1050616_nation_story_4874434.utf8\n",
      "115829    1070918_nation_story_8329881.utf8\n",
      "40673     1050705_nation_story_4950117.utf8\n",
      "40468     1050622_nation_story_4899989.utf8\n",
      "Name: DOCNO, dtype: object\n",
      "\n",
      "Relevant documents: \n",
      "1041102_nation_story_3954727.utf8\n",
      "1041105_nation_story_3968019.utf8\n",
      "1050709_nation_story_4968974.utf8\n",
      "\n",
      "Number of relevant documents retrieved:  0\n",
      "Relevant documents retrieved using LSI:\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Query Number:  79\n",
      "Documents retrieved using LSI: \n",
      "114437     1070621_nation_story_7952530.utf8\n",
      "80844      1060902_nation_story_6690181.utf8\n",
      "82168      1061121_nation_story_7029464.utf8\n",
      "79735      1060630_nation_story_6418918.utf8\n",
      "45912     1051107_opinion_story_5432707.utf8\n",
      "Name: DOCNO, dtype: object\n",
      "\n",
      "Relevant documents: \n",
      "1070621_frontpage_story_7952567.utf8\n",
      "1070621_nation_index.utf8\n",
      "1070621_nation_story_7952530.utf8\n",
      "1070621_nation_story_7952531.utf8\n",
      "\n",
      "Number of relevant documents retrieved:  1\n",
      "Relevant documents retrieved using LSI:\n",
      "1070621_nation_story_7952530.utf8\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Query Number:  80\n",
      "Documents retrieved using LSI: \n",
      "44813      1050617_opinion_story_4875891.utf8\n",
      "35978    1050530_frontpage_story_4803595.utf8\n",
      "40409       1050618_nation_story_4884165.utf8\n",
      "36515    1050909_frontpage_story_5216722.utf8\n",
      "44768      1050611_opinion_story_4853543.utf8\n",
      "Name: DOCNO, dtype: object\n",
      "\n",
      "Relevant documents: \n",
      "1041103_frontpage_story_3959012.utf8\n",
      "1041220_nation_story_4151396.utf8\n",
      "1041223_frontpage_story_4163563.utf8\n",
      "1041228_nation_story_4182211.utf8\n",
      "1050707_nation_story_4959905.utf8\n",
      "1050729_nation_story_5048357.utf8\n",
      "1050916_nation_story_5245081.utf8\n",
      "1070323_nation_story_7555002.utf8\n",
      "\n",
      "Number of relevant documents retrieved:  0\n",
      "Relevant documents retrieved using LSI:\n",
      "\n",
      "\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in query_relevant_documents.keys():\n",
    "    print(\"Query Number: \",i)\n",
    "    print(\"Documents retrieved using LSI: \")\n",
    "    print(query_relevant_documents[i][1])\n",
    "    print(\"\\nRelevant documents: \")\n",
    "    for q in query_rel_doc[i]:\n",
    "        print(q)\n",
    "    lsi_retrieved = set(query_relevant_documents[i][1])\n",
    "    act_docs = set(query_rel_doc[i])\n",
    "    print(\"\\nNumber of relevant documents retrieved: \",len(lsi_retrieved.intersection(act_docs)))\n",
    "    print(\"Relevant documents retrieved using LSI:\")\n",
    "    for d in lsi_retrieved.intersection(act_docs):\n",
    "        print(d)\n",
    "    print(\"\\n\")\n",
    "    print(\"*\"*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query No: 77, 3 relevant documents are retrieved \n",
    "# Query No: 79, 1 relevant document is retrieved\n",
    "# when LSI was implemented by using truncated SVD of TFIDF-MATRIX with 250 components"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
